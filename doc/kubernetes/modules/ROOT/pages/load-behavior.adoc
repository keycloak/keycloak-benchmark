= Keycloak under load
:description: This topic describes how Keycloak behaves under a high load.

{description}
It also offers configuration best practices to prepare Keycloak for a high load.

== Database connections

Creating new database connections is expensive as it takes time.
Creating them when a request arrives will delay the response, so it is good to have them created before the request arrives.
It can also contribute to a https://en.wikipedia.org/wiki/Cache_stampede[stampede effect] where creating a lot of connections in a short time makes things worse as it slows down the system and blocks threads.
Closing a connection also invalidates all service side statements caching for that connection.

include::partial$configure-db-connection-pool-best-practices.adoc[]

See xref:customizing-deployment.adoc#KC_DB_POOL_INITIAL_SIZE[the Keycloak deployment configuration options `KC_DB_*`]

[#threads]
== Threads

Starting with Keycloak 20.0.0 and running Quarkus, all Keycloak requests are dispatched to the executor pool of Quarkus (see https://github.com/keycloak/keycloak/pull/15193[keycloak#15193]).

Before that change, regular Keycloak requests were handled on the Vert.x worker pool and could therefore exhaust it, the liveness probe could fail, and Keycloak pods would restart under a high load.

Since this change, the threads behave as follows:

* All Keycloak requests are handled on the Quarkus executor pool.
This is configured in https://quarkus.io/guides/all-config#quarkus-core_quarkus.thread-pool.max-threads[`quarkus.thread-pool.max-threads`] and has a maximum size of at least 200 threads.
Depending on the available CPU cores it can grow even larger.
Threads are created as needed, and will end when no longer needed, so the system will scale up and down as needed.
+
When the load and the number of threads increases, the bottleneck will usually be the database connections.
Once a request can't acquire a database connection, it will fail with a message in the log like `Unable to acquire JDBC Connection` or similar as described in xref:error-messages.adoc#keycloak-message-error-failed-to-obtain-jdbc-connection[the known error messages].
The caller will receive a response with a 5xx HTTP status code indicating a server side error.
+
With the number of threads in the executor pool being an order of magnitude larger than the number of database connections and with requests failing when no database connection is available within the https://quarkus.io/guides/all-config#quarkus-agroal_quarkus.datasource.jdbc.acquisition-timeout[`quarkus.datasource.jdbc.acquisition-timeout`] (5 seconds default), this is somewhat of a https://en.wikipedia.org/wiki/Demand_response#Load_shedding[load-shedding behavior] where it returns an error response instead of queueing requests for an indefinite amount of time.
+
The combined number of executor threads in all Keycloak nodes in the cluster shouldn't exceed the number of threads available in JGroups thread pool to avoid the error described in
xref:kubernetes-guide::error-messages.adoc#jgroups-thread-pool-is-full['org.jgroups.util.ThreadPool: thread pool is full'].
+
--
include::partial$executor-jgroups-thread-calculation.adoc[]
--
+
Configure `quarkus.thread-pool.queue-size` to specify a maximum queue length to allow for effective load shedding once this queue size is exceeded: Keycloak will return HTTP Status code 503 (server unavailable) in this situation (available in Keycloak 23, see https://github.com/keycloak/keycloak/issues/21962[#21962] for details).
Assuming a Keycloak pod processes around 200 requests per second, a queue of 1000 would lead to maximum waiting times of around 5 seconds.

* All health and liveness probes are handled in the Vert.x worker pool.
This is configured in https://quarkus.io/guides/all-config#quarkus-vertx_quarkus.vertx.worker-pool-size[`quarkus.vertx.worker-pool-size`] and has a default of 20 threads.
As it is independent of requests queueing up in the executor pool, health and liveness will still be handled fast and without a delay, so it is safe to use them in Kubernetes without having pods restarted under a high load.
+
There should be no need to configure a specific value for this.

In order for Java to create threads, when running on Linux it needs to have file handles available.
Therefore, the number of open files (as retrieved as `ulimit -n` on Linux) need to provide head-space for Keycloak to increase the number of threads needed.
Each thread will also consume memory, and the container memory limits need to be set to a value that allows for this or the pod will be killed by Kubernetes.
