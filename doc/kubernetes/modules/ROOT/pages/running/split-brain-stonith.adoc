= Deploy an AWS Lambda to guard against Split-Brain
:description: This guide explains how to reduce the impact when split-brain scenarios occur between two sites in an Active/Active deployment.

{description}

== Architecture
In the event of a network communication failure between the two sites in a Multi-AZ Active/Active deployment, it is no
longer possible for the two sites to continue to replicate session state between themselves and the two sites
will become increasingly out-of-sync. As it's possible for subsequent Keycloak requests to be routed to different
sites, this may lead to unexpected behaviour as previous updates will not have been applied to both sites.

Typically in such scenarios a quorum is used to determine which sites are marked as online or offline, however as Active/Active
deployments only consist of two sites, this is not possible. Instead, we leverage the STONITH (Shoot The Other In The Head)
pattern to ensure that once a split-brain has been detected, only one site can continue to serve user requests.

STONITH is implemented by using https://prometheus.io/docs/alerting/latest/overview/[Prometheus Alerts] to call an AWS
Lambda based webhook whenever one of the sites is unable to establish or connect to the other side. The triggered Lambda
function inspects the current Global Accelerator configuration and removes the site reported to be offline.

In a true split-brain scenario, where both sites are still up but network communication is down, it's probable that both
sites will trigger the webhook. We guard against this by ensuring that only a single Lambda instance can be executed at
a given time.

== Prerequisites

* ROSA HCP based Multi-AZ Keycloak deployment
* AWS Global Accelerator Loadbalancer

== Procedure
. Enable Openshift user alert routing
+
.Command:
[source,bash]
----
kubectl apply -f - << EOF
apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    alertmanager:
      enabled: true
      enableAlertmanagerConfig: true
EOF
kubectl -n openshift-user-workload-monitoring rollout status --watch statefulset.apps/alertmanager-user-workload
----
+
. Create the Role used to execute the Lambda.
+
.Command:
[source,bash]
----
FUNCTION_NAME= # <1>
ROLE_ARN=$(aws iam create-role \
  --role-name ${FUNCTION_NAME} \
  --assume-role-policy-document \
  '{
    "Version": "2012-10-17",
    "Statement": [
      {
        "Effect": "Allow",
        "Principal": {
          "Service": "lambda.amazonaws.com"
        },
        "Action": "sts:AssumeRole"
      }
    ]
  }' \
  --query 'Role.Arn' \
  --region eu-west-1 \#<2>
  --output text
)
----
<1> A name of your choice to associate with the Lambda and relataed resources
<2> The AWS Region hosting your Kubernetes clusters
+
. Attach the `ElasticLoadBalancingReadOnly` policy so that the Lambda can query the provisioned Network Load Balancers
+
.Command:
[source,bash]
----
aws iam attach-role-policy \
  --role-name ${FUNCTION_NAME} \
  --policy-arn arn:aws:iam::aws:policy/ElasticLoadBalancingReadOnly
----
+
. Attach the `GlobalAcceleratorFullAccess` policy so that the Lambda can update the Global Accelerator EndpointGroup
+
.Command:
[source,bash]
----
aws iam attach-role-policy \
  --role-name ${FUNCTION_NAME} \
  --policy-arn arn:aws:iam::aws:policy/GlobalAcceleratorFullAccess
----
+
. Create a Lambda ZIP file containing the required STONITH logic
+
.Command:
[source,bash]
----
LAMBDA_ZIP=/tmp/lambda.zip
cat << EOF > /tmp/lambda.py

include::example$stonith_lambda.py[]

EOF
zip -FS --junk-paths ${LAMBDA_ZIP} /tmp/lambda.py
----
+
. Create the Lambda function.
+
.Command:
[source,bash]
----
aws lambda create-function \
  --function-name ${FUNCTION_NAME} \
  --zip-file fileb://${LAMBDA_ZIP} \
  --handler lambda.handler \
  --runtime python3.12 \
  --role ${ROLE_ARN} \
  --region eu-west-1 #<1>
----
<1> The AWS Region hosting your Kubernetes clusters
+
. Expose a Function URL so the Lambda can be triggered as webhook
+
.Command:
[source,bash]
----
aws lambda create-function-url-config \
  --function-name ${FUNCTION_NAME} \
  --auth-type NONE \
  --region eu-west-1 #<1>
----
<1> The AWS Region hosting your Kubernetes clusters
+
. Allow public invocations of the Function URL
+
.Command:
[source,bash]
----
aws lambda add-permission \
  --action "lambda:InvokeFunctionUrl" \
  --function-name ${FUNCTION_NAME} \
  --principal "*" \
  --statement-id FunctionURLAllowPublicAccess \
  --function-url-auth-type NONE \
  --region eu-west-1 #<1>
----
<1> The AWS Region hosting your Kubernetes clusters

== Verify

To test that the Prometheus alert triggers the webhook as expected, perform the following steps to simulate a split-brain:

. In each of your clusters execute the following:
+
.Command:
[source,bash]
----
kubectl -n openshift-operators scale --replicas=0 deployment/infinispan-operator-controller-manager #<1>
kubectl -n openshift-operators rollout status -w deployment/infinispan-operator-controller-manager
kubectl -n ${NAMESPACE} scale --replicas=0 deployment/infinispan-router #<2>
kubectl -n ${NAMESPACE} rollout status -w deployment/infinispan-router
----
<1> Scale down the Infinispan Operator so that <2> does not result in the deployment being recreated by the operator
<2> Scale down the Gossip Router deployment.Replace `$\{NAMESPACE}` with the namespace containing your Infinispan server
+
. Verify the `SiteOffline` event has been fired on a cluster by executing <TODO>
+
. Inspect the Global Accelerator EndpointGroup in the AWS console and there should only be a single endpoint present
+
. Scale up the Infinispan Operator and Gossip Router to re-establish a connection between sites:
+
.Command:
[source,bash]
----
kubectl -n openshift-operators scale --replicas=1 deployment/infinispan-operator-controller-manager
kubectl -n openshift-operators rollout status -w deployment/infinispan-operator-controller-manager
kubectl -n ${NAMESPACE} scale --replicas=1 deployment/infinispan-router #<1>
kubectl -n ${NAMESPACE} rollout status -w deployment/infinispan-router
----
<1> Replace `$\{NAMESPACE}` with the namespace containing your Infinispan server
+
. Inspect the `vendor_jgroups_site_view_status` metric in each site. A value of `1` indicates that the site is reachable.
+
. Update the Accelerator EndpointGroup to contain both Endpoints <TODO add link to operating procedure>
