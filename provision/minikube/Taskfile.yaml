# https://taskfile.dev

version: '3'

output: prefixed

includes:
  common: ../common

vars:
  IP:
    sh: minikube ip

dotenv: ['.env']

tasks:
  default:
    deps:
      - gatlinguser
    cmds:
      - echo Keycloak is ready for load testing!
      - bash -c ./isup.sh
    silent: true

  ipchange:
    deps:
      - common:split
    cmds:
      - rm -f .task/checksum/keycloak
      - rm -f .task/checksum/prometheus
      - rm -f .task/checksum/monitoring
      - mkdir -p .task
      - echo -n {{.IP}} > .task/status-{{.TASK}}
    sources:
      - .task/subtask-{{.TASK}}.yaml
    status:
      - test -e .task/status-{{.TASK}}
      - test "{{.IP}}" == "$(cat .task/status-{{.TASK}})"
    # avoid 'once' until https://github.com/go-task/task/issues/715 when running with parameter '-C 1'
    run: once

  reset-keycloak:
    deps:
      - common:split
    cmds:
      - bash -c "kubectl delete keycloaks.k8s.keycloak.org/keycloak -n keycloak || exit 0"
      - bash -c "kubectl delete deployment/keycloak-operator -n keycloak || exit 0"
      - kubectl wait --timeout=30s -n keycloak --for delete pod --selector=app=keycloak
      - kubectl wait --timeout=30s -n keycloak --for delete pod --selector=app.kubernetes.io/name=keycloak-operator
      - >
        bash -c '
        if [ "{{.KC_DATABASE}}" == "postgres+infinispan" ];
        then kubectl delete deployment/postgres -n keycloak || exit 0;
             kubectl delete deployment/infinispan -n keycloak || exit 0;
        else
             kubectl delete deployment/{{.KC_DATABASE}} -n keycloak || exit 0;
        fi'
      - kubectl wait --timeout=30s -n keycloak --for delete pod --selector=app=postgres
      - ./init-database.sh {{.KC_DATABASE}} reset
      # discard status of keycloak to force redeployment
      - rm -f .task/checksum/keycloak
      # discard status of gatling user to force redeployment
      - rm -f .task/checksum/gatlinguser
      - task: default

  prometheus:
    deps:
      - common:split
      - ipchange
      - common:env
    cmds:
      - kubectl create namespace monitoring || true
      - helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
      - helm repo update
      - helm upgrade --install prometheus prometheus-community/kube-prometheus-stack --version 39.11.0 -f monitoring.yaml --set grafana."grafana\.ini".server.root_url=https://grafana.{{.IP}}.nip.io --set prometheus.prometheusSpec.retention={{.KB_RETENTION}}
    sources:
      - monitoring.yaml
      - .task/subtask-{{.TASK}}.yaml
      - .task/var-KB_RETENTION
    run: once

  monitoring:
    deps:
      - prometheus
      - common:split
      - ipchange
    cmds:
      - helm upgrade --install monitoring --set hostname={{.IP}}.nip.io monitoring
    sources:
      - monitoring/**/*.*
      - .task/subtask-{{.TASK}}.yaml

  jaeger:
    deps:
      - common:split
      - prometheus
      - common:env
    cmds:
      - helm repo add jaegertracing https://jaegertracing.github.io/helm-charts
      - helm repo update
      - >
        helm upgrade --install jaeger jaegertracing/jaeger --version 3.1.2 -n monitoring -f ../minikube/jaeger/values.yaml
        --set allInOne.extraEnv[0].value={{.KB_RETENTION}}
    sources:
      - jaeger/**/*.*
      - .task/subtask-{{.TASK}}.yaml
      - .task/var-KB_RETENTION

  tempo:
    deps:
      - common:split
      - prometheus
    cmds:
      # For now, Jaeger stores the traces, and Tempo is disabled.
      # - helm repo add grafana https://grafana.github.io/helm-charts
      # - helm repo update
      # - helm upgrade --install tempo grafana/tempo -n monitoring -f tempo.yaml
      - helm delete tempo -n monitoring || exit 0
    sources:
      - tempo.yaml
      - .task/subtask-{{.TASK}}.yaml

  loki:
    deps:
      - common:split
      - common:env
      - prometheus
    cmds:
      - helm repo add grafana https://grafana.github.io/helm-charts
      - helm repo update
      # A loki update might fail as a stateful set can't be updated. If that fails, uninstall and re-install.
      - >
        bash -c "helm upgrade --install loki grafana/loki --version 3.0.0 -n monitoring -f loki.yaml --set config.table_manager.retention_period={{.KB_RETENTION}}
        || (helm delete loki -n monitoring && helm upgrade --install loki grafana/loki --version 3.0.0 -n monitoring -f loki.yaml --set config.table_manager.retention_period={{.KB_RETENTION}})"
    sources:
      - loki.yaml
      - .task/subtask-{{.TASK}}.yaml
      - .task/var-KB_RETENTION

  promtail:
    deps:
      - common:split
      - prometheus
    cmds:
      - helm repo add grafana https://grafana.github.io/helm-charts
      - helm repo update
      - helm upgrade --install promtail grafana/promtail --version 6.3.0 -n monitoring -f promtail.yaml
    sources:
      - promtail.yaml
      - .task/subtask-{{.TASK}}.yaml

  dataset-import:
    deps:
      - gatlinguser
    cmds:
      - bash -c "../../dataset/dataset-import.sh {{.CLI_ARGS}}"
    silent: true

  gatlinguser:
    deps:
      - common:keycloak-cli-unzip
      - common:tlsdisableagent
      - keycloak
      - common:split
      - ipchange
    env:
      KC_OPTS: "-javaagent:../tlsdisableagent/tlscheckdisable-agent.jar"
      KEYCLOAK_HOME: "../keycloak-cli/keycloak"
    cmds:
      - bash -c ./isup.sh
      - bash -c "../keycloak-cli/keycloak/bin/kcadm.sh config credentials --server https://keycloak-keycloak.{{.IP}}.nip.io/ --realm master --user admin --password admin"
      - bash -c "../../benchmark/src/main/content/bin/initialize-benchmark-entities.sh -r test-realm -d"
    sources:
      - ../../benchmark/src/main/content/bin/initialize-benchmark-entities.sh
      - .task/subtask-{{.TASK}}.yaml
      # if keycloak's database deployment changes, this restarts the DB and the Gatling user needs to be re-created
      - .task/status-keycloak-db.json
      - .task/var-KC_DATABASE

  keycloak:
    preconditions:
      - sh: '[ "{{.KC_DATABASE}}" != "aurora-postgres" ]'
        msg: "KC_DATABASE='{{.KC_DATABASE}}' not supported on minikube"
    deps:
      - monitoring
      - common:datasetprovider
      - common:split
      - ipchange
      - jaeger
      - loki
      - tempo
      - promtail
      - common:env
    cmds:
      - >
      - kubectl create namespace keycloak || true
      - kubectl -n keycloak apply -f https://raw.githubusercontent.com/keycloak/keycloak-k8s-resources/refs/tags/{{.KC_OPERATOR_TAG}}/kubernetes/keycloaks.k8s.keycloak.org-v1.yml
      - kubectl -n keycloak apply -f https://raw.githubusercontent.com/keycloak/keycloak-k8s-resources/refs/tags/{{.KC_OPERATOR_TAG}}/kubernetes/keycloakrealmimports.k8s.keycloak.org-v1.yml
      - kubectl -n keycloak apply -f https://raw.githubusercontent.com/keycloak/keycloak-k8s-resources/refs/tags/{{.KC_OPERATOR_TAG}}/kubernetes/kubernetes.yml || (kubectl -n keycloak delete deployment/keycloak-operator && kubectl -n keycloak apply -f https://raw.githubusercontent.com/keycloak/keycloak-k8s-resources/refs/tags/{{.KC_OPERATOR_TAG}}/kubernetes/kubernetes.yml)
      - >
        helm upgrade --install keycloak
        --set hostname={{.IP}}.nip.io
        --set otel={{.KC_OTEL}}
        --set otelSamplingPercentage={{.KC_OTEL_SAMPLING_PERCENTAGE}}
        --set dbPoolInitialSize={{.KC_DB_POOL_INITIAL_SIZE}}
        --set dbPoolMinSize={{.KC_DB_POOL_MIN_SIZE}}
        --set dbPoolMaxSize={{.KC_DB_POOL_MAX_SIZE}}
        --set database={{.KC_DATABASE}}
        --set keycloakImage={{.KC_CONTAINER_IMAGE}}
        --set instances={{ .KC_INSTANCES }}
        --set cpuRequests={{ .KC_CPU_REQUESTS }}
        --set cpuLimits={{ .KC_CPU_LIMITS }}
        --set memoryRequestsMB={{ .KC_MEMORY_REQUESTS_MB }}
        --set memoryLimitsMB={{ .KC_MEMORY_LIMITS_MB }}
        --set heapInitMB={{ .KC_HEAP_INIT_MB }}
        --set heapMaxMB={{ .KC_HEAP_MAX_MB }}
        --set metaspaceInitMB={{ .KC_METASPACE_INIT_MB }}
        --set metaspaceMaxMB={{ .KC_METASPACE_MAX_MB }}
        --set cryostat={{ .KC_CRYOSTAT }}
        --set infinispan.customConfig={{ .KC_CUSTOM_INFINISPAN_CONFIG }}
        --set infinispan.configFile={{ .KC_CUSTOM_INFINISPAN_CONFIG_FILE }}
        --set infinispan.remoteStore.enabled={{ .KC_REMOTE_STORE }}
        --set infinispan.remoteStore.host={{ .KC_REMOTE_STORE_HOST }}
        --set infinispan.remoteStore.port={{ .KC_REMOTE_STORE_PORT }}
        keycloak
      - >
        bash -c '
        if [ "{{.KC_DATABASE}}" != "none" ];
        then kubectl get deployment/{{.KC_DATABASE}} -n keycloak -o=jsonpath="{.spec}" > .task/status-{{.TASK}}-db.json;
        else echo "none" > .task/status-{{.TASK}}-db.json;
        fi'
      # kill all CrashLoopBackOff and ImagePullBackOff pods to trigger a fast restart and not wait Kubernetes
      - bash -c 'kubectl get pods -A | grep -E "(BackOff|Error|ErrImageNeverPull|InvalidImageName)" | tr -s " " | cut -d" " -f1-2 | xargs -r -L 1 kubectl delete pod -n'
      # wait a bit for the operator to pick up the changes
      - bash -c 'sleep 2'
      - ./init-database.sh {{.KC_DATABASE}}
      - ./isup.sh
      # remove all no longer used images from minikube to preserve disk space
      # "minikube ssh" stopped to work on Fedora / lead to a timeout, while direct ssh still works
      - bash -c 'ssh docker@$(minikube ip) -i $(minikube ssh-key) -o StrictHostKeyChecking=no docker container prune -f'
      - bash -c 'ssh docker@$(minikube ip) -i $(minikube ssh-key) -o StrictHostKeyChecking=no docker image prune -f'
      - bash -c 'ssh docker@$(minikube ip) -i $(minikube ssh-key) -o StrictHostKeyChecking=no docker volume prune -f'
    sources:
      - keycloak/**/*.*
      - .task/subtask-{{.TASK}}.yaml
      - .task/var-KC_*
