= Provision Keycloak for a benchmark test with minikube
:experimental:
:icons: font

== About

This describes how to install Keycloak and a Grafana/Prometheus/Jaeger monitoring stack.

This uses minikube to run the containers, and Helm to provision the containers.
Keycloak itself is provisioned using the Java based Keycloak operator.
Kubectl is used to interact with minikube on the command line.
https://github.com/astefanutti/kubebox[Kubebox] is installed for a minimal UI to inspect containers within minikube.

The helm charts for Keycloak can be configured to just deploy Kubernetes without the dependency on the PodMonitor and ServiceMonitor dependencies.

This setup has an optional OpenTelemetry (OTEL) setup that will gather additional metrics and will publish traces from within Keycloak to Jaeger service.
This allows measuring the latency per endpoint, and tracing of the database statements executed by a given REST endpoint.
It also collects the logs of all containers inside Loki that can be queried from Grafana.

It also contains Tempo as an alternative tracing option.
While traces are submitted via OTEL successfully and the search by trace ID works as expected, the search for traces (currently beta) doesn't return some traces (for example deletion of users).

== Alternatives considered

This paragraph describes some possible alternatives and how they differ from the current approach:

Docker Compose::
+
--
* This setup would have been more minimal, as it wouldn't require a virtual machine for its setup.

* Docker Compose would support CPU limits when running real Docker, but not Podman.

* Docker Compose doesn't support templating so customizing different setups with different CPU limits is difficult while such templating is available with Helm for Minikube.
For the Grafana stack there is a good customizable Helm template with no such way for Docker Compose.
--
+
*Decision:* Go with Minikube, while still keeping Docker Compose as a minimal setup for ad-hoc testing and to allow for a small effort solution for the community.
(Proposed by Alexander Schwartz in May 2022)
+
*Scope:* Docker Compose will contain only Keycloak and a database, and will not contain a monitoring stack, and will not impose any CPU limits.

OpenShift Local (formerly known as CodeReady Containers)::
+
--
* It would have the same capabilities as Minishift, with operator support already installed.
It could be setup automatically (if the developer registers for a Red Hat account and would get a pull secret).

* OpenShift style Monitoring could be installed either via the standard monitoring functionality/operators, and possibly with additional extensions.
On the other hand, the helm charts seem to be more configurable than the OpenShift operator.

* The team of the Keycloak Java operator is working with Minikube to test it, and the operator hub functionality can be installed within minutes with a shell script.
This would allow for running the Keycloak operator on Minishift for this setup as well.

* OpenShift Local will always use a VM, and will be more heavyweight in terms of CPU and RAM usage.
Installing OpenShift is a bigger step for a community contributor.
In the long term, maintaining the setup for both OpenShift and Minikube might be a higher maintenance cost.
Installing OpenShift Local from scratch takes a lot longer than installing Minishift, as OpenShift comes with a lot more Operators that need to download and start their containers and will wait for dependencies.

* Minikube has a mechanism to build containers in a lightweight way locally and provide them to the running Minikube instance.
The alternative for OpenShift would be https://docs.openshift.com/container-platform/4.10/cicd/builds/creating-build-inputs.html#builds-binary-source_creating-build-inputs[binary builds].
--
+
*Decision:* Go with Minikube for now.
Add additional parameterization to the Helm scripts later where needed to deploy on OpenShift (either Local or regular).
Revisit the decision later once the first OpenShift deployments have been made.
(Proposed by Alexander Schwartz in May 2022)

== Present and future

This has been set up for a local developer setup.
Kubebox, Prometheus and Grafana don't require authentication, therefore it shouldn't be installed in an environment that is accessible from remote.

This project should eventually evolve to a setup with multiple namespaces to represent different datacenters.

== Limitations

For Linux, the kvm2 driver is needed for a scalable solution (tested 15 Keycloak replicas).
More instances are possible when adding more than 8 GB of RAM.

The podman driver on Linux currently supports at the moment only up to 5 instances of Keycloak due to the number of open files limit that is actually a limit of the number of threads.
After that, the containers will complain that they're unable to start more processes.

=== Cryostat for JFR recordings

The contents of the helm chart have been created originally by the Cryostat operator.
When analyzing the resources created by the operator in version 2.0, there was no supported way to add the environment variables needed to the cryostat-deployment discovered.
Due to that, this has now been extracted and placed here as a Helm chart.

The Cryostat instance needs to run in the same namespace as the JVMs it connects to.
Due to that, it is part of the Keycloak deployment, and not a separate Helm chart.

The profiling created is regular profiling, not async profiling. The profiling will therefore suffer from the safepoint bias problem. See the https://github.com/jvm-profiling-tools/async-profiler#async-profiler[Java async profiler] for details:

[quote, Java async profiler]
____
This project is a low overhead sampling profiler for Java that does not suffer from http://psy-lob-saw.blogspot.com/2016/02/why-most-sampling-java-profilers-are.html[Safepoint bias problem]. It features HotSpot-specific APIs to collect stack traces and to track memory allocations. The profiler works with OpenJDK, Oracle JDK and other Java runtimes based on the HotSpot JVM.
____

For now, not using async profiling should be good enough until proven otherwise.

== Architecture

This describes the runtime view of the setup:

.Minikube runtime view
image::minikube-runtime-view.dio.svg[]

The setup is as follows:

* https://minikube.sigs.k8s.io/[Minikube] runs a virtual machine.
* Via a configured ingress, a local browser can access different services running in Minikube like Keycloak and Grafana.
* https://www.keycloak.org/[Keycloak] connects to a PostgreSQL database running inside Minikube.
* https://www.postgresql.org/[The PostgreSQL database] inside minikube is accessible via a node port from the host.
* https://prometheus.io/[Prometheus] collects metrics, and Jaeger collects traces.
* https://grafana.com/docs/loki/latest/clients/promtail/[Promtail] collects logs and sends it to Loki which stores them.
* https://gatling.io/[Gatling] can run locally and send Graphite metrics via a node port to a collector inside Minikube.
* https://www.jaegertracing.io/[Jaeger] collects traces from Keycloak running inside Minikube, and can also receive traces from a locally running test application.
* https://cryostat.io/[Cryostat] can connect to Keycloak instances and create Java Flight Recorder (JFR) recordings.

== Prerequisites

The following needs to be installed on the local machine:

* https://minikube.sigs.k8s.io/docs/start/[Minikube]
* https://helm.sh/docs/intro/install/[Helm]
* https://kubernetes.io/docs/tasks/tools/[kubectl]

The installation can be performed on Linux as follows:

. Download each executable and place it in ~/bin
. Add the following snippet to ~/.bashrc to allow auto-completion of commands
+
----
source <(minikube completion bash)
source <(helm completion bash)
source <(kubectl completion bash)
----

////
Not needed for kvm2 driver

Increase the number of files by adding the following to `/etc/systemd/system.conf` and `/etc/systemd/user.conf`:

----
DefaultLimitNOFILE=102400:524288
----

Test the settings afterwards using `ulimit -n`, it should match the first value.

WARNING: There still seems to eb a limit of around ~2k container threads in total that prevents more than 5 running instances of Keycloak.
////

== Installation

=== For the impatient

The installation has been scripted in `rebuild.sh`.
If an existing minikube instance exists, it will destroy it first.
Run this script, and see the URLs printed in the console to access the different services.

Wait a bit for all containers to be pulled from the internet, then get started.

The following commands helps to watch the pods being started, use kbd:[Ctrl+C] to end watching.

[source,shell]
----
kubectl get pods -A -w
----

The following script will check if all services are running and will output a list of available URLs.

[source,shell]
----
./isup.sh
----

To update an existing Minikube setup created with an earlier version of this project, use `upgrade.sh`.
It will install all changes in the Helm charts and Grafana charts.

To open a dashboard showing all Kubernetes resources, run the following command:

[source,shell]
----
minikube dashboard
----

This should open the URL in your default browser.
If it doesn't open it automatically, click on the link it prints on the console.

Then, select a namespace in the header (for example `keycloak`) and browse the resources available in that namespace.

=== For more insights and backgrounds

This section will show the different steps with variants, explain them a bit more.
It also shows the `helm upgrade` commands that can update parts of the stack incrementally which helps development and upgrades.

Startup Minikube in default mode with a VM.
Per default, it will use 2 CPUs, and this can be adjusted

[source,shell]
----
minikube start
----

Start with customized settings.

[source,shell]
----
minikube stop
minikube delete
minikube start --memory 8192 --cpus 4
----

Depending on the driver, adjusting the settings might work for an already created minikube instance.

[source,shell]
----
minikube stop
minikube config set memory 8192
minikube config set cpus 4
minikube start
----

Startup Minikube on Linux w/ podman driver.
This allows faster startup times, less overhead, and no limitation (?) on CPU usage.

////
Installation of cri-o not needed, cri-o will run inside the minikube podman?
dnf module enable cri-o:1.19
dnf install cri-o
////

[source,shell]
----
minikube start --driver=kvm2 --docker-opt="default-ulimit=nofile=102400:102400"
----

This requires libvirtd to run.

----
sudo systemctl enable libvirtd
sudo systemctl start libvirtd
sudo usermod -a -G libvirt $USER
# now relogin, for usermod to become effective
----

For a lightweight installation that today doesn't scale beyond 3-5 Keycloak instances:

[source,shell]
----
minikube start --driver=podman --container-runtime=cri-o
----

On Linux, allow to use podman and crio via sudo:

. run `sudo visudo`
. add the following to the sudoer's file
+
----
username ALL=(ALL) NOPASSWD: /usr/bin/podman
username ALL=(ALL) NOPASSWD: /usr/bin/crictl
----

Adding ingress

[source,shell]
----
minikube addons enable ingress
----

Install Prometheus and Grafana.

[source,shell]
----
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo update
kubectl create namespace monitoring
helm upgrade --install prometheus prometheus-community/kube-prometheus-stack --set grafana."grafana\.ini".server.root_url=https://grafana.$(minikube ip).nip.io -f monitoring.yaml
----

Install Customizations for Grafana, including an ingress for minikube.
Login to Grafana with admin / keycloak unless anonymous login is enabled.

Custom dashboards are included in folder `monitoring/dashbaords`.
Add more dashboards there as new files, and a `helm update` will install the latest versions in the minikube cluster.

[source,shell]
----
helm upgrade --install monitoring --set hostname=$(minikube ip).nip.io monitoring
----

Install Keycloak including monitoring.

Set `monitoring` to `false` to install Keycloak without monitoring options.

Set `otel` to `true` to install Keycloak with opentelemetry enabled.

[source,shell]
----
kubectl create namespace keycloak
kubectl -n keycloak apply -f https://raw.githubusercontent.com/keycloak/keycloak-k8s-resources/nightly/kubernetes/keycloaks.k8s.keycloak.org-v1.yml
kubectl -n keycloak apply -f https://raw.githubusercontent.com/keycloak/keycloak-k8s-resources/nightly/kubernetes/keycloakrealmimports.k8s.keycloak.org-v1.yml
kubectl -n keycloak apply -f https://raw.githubusercontent.com/keycloak/keycloak-k8s-resources/nightly/kubernetes/kubernetes.yml
helm upgrade --install keycloak --set hostname=$(minikube ip).nip.io keycloak
----

Add Tempo for tracing

[source,shell]
----
helm repo add grafana https://grafana.github.io/helm-charts
helm search repo grafana
helm upgrade --install tempo grafana/tempo -n monitoring -f tempo.yaml
----

Add Loki to store information about logs, and promtail to collect the logs from all containers.

[source,shell]
----
helm upgrade --install loki grafana/loki -n monitoring -f loki.yaml
helm upgrade --install promtail grafana/promtail -n monitoring -f promtail.yaml
----

Installing Jaeger as a tracing solution

[source,shell]
----
helm repo add jaegertracing https://jaegertracing.github.io/helm-charts
helm upgrade --install jaeger jaegertracing/jaeger -n monitoring -f jaeger.yaml
----

== Pause/Resume setup

The setup can be paused and resumed without restarting/reinstalling all pods.

To stop, run the following command:

[source,bash]
----
minikube stop
----

To resume, run the following command and specify the driver you used when running minikube originally.

[source,bash]
----
minikube start --driver=...
----

After minikube has been re-started, it might have a different IP address for the ingress.
Due to that, all ingresses need to be updated.
Do this by running helm:

[source,bash]
----
helm upgrade monitoring --set hostname=$(minikube ip).nip.io monitoring
helm upgrade keycloak --set hostname=$(minikube ip).nip.io keycloak
----

== Running `kcadm.sh` with invalid TLS certificates

The minikube setup doesn't contain trusted TLS certificates, and the certificates will also not match the hostnames.

To disable the TLS checks in Java, see the module `provision/tlsdisableagent` for details on how to run for example `kcadm.sh`.

== Accessing the PostgreSQL database inside minikube

To access the PostgreSQL database running inside minikube, there are the following options:

* Execute a shell using `kubectl`:
+
----
kubectl exec `kubectl get pods --selector=app=postgres -n keycloak -o name` -n keycloak -it -- psql --user keycloak
----

* Open the web-based sqlpad pod. Run the `isup.sh` shell script to see the URL. +
Log in with username `admin` and password `admin`.

* Connect via a local DB client:
+
--
. Retrieve minikube's IP address using `minikube ip`
. Assuming that the IP-address is `192.168.39.39`, point your DB tool at the JDBC URL `jdbc:postgresql://192.168.39.39:30009/keycloak`.
+
The connection details: Port will always be `30009`, username is `keycloak`, password is `pass`, database name is `keycloak`.
--
+
NOTE: Minikube's IP address will change every time you re-create the minikube instance.

== Creating a Java Flight Recorder recording

* Open the Cryostat instance's website. Run the `isup.sh` shell script to see the URL.
* Click on the menu item menu:Recordings[].
* Select a target VM.
* Click on button btn:[Create] to create a new recording and follow the dialogs.

Once the recording is complete, download it directly or archive it to the persistent volume of Cryostat to download it later.

== Running Gatling

To run the benchmarks using Gatling on your local machine and to forward the metrics to the Graphite exporter in Minikube, you'll need to pass the IP-address of Minikube as an environment variable that is then used inside `gatling.conf`.

[source,bash]
----
export GRAPHITE_TCP_ADDR=$(minikube ip)
----

The mapping of Gatling's metrics to Prometheus a metric name and labels is configured in `graphite_mapping.yaml`.
Once the test runs, the metrics are available as `gatling_users` and `gatling_requests`.

This setup assumes that only one load driver is running.
If more load drivers are running, change the `rootPathPrefix` in Gatling's configuration and the `gatling.conf` setup need to change.
For now, this is considered out-of-scope as one Gatling instance can generate several orders of magnitude more load than needed.

The Prometheus Gatling exporter will hold the metrics for 5 minutes and then forget them.
By that time, Prometheus will have already scraped them and stored the values in its database.

== Connecting to a remote host running minikube

When running minikube on a remote host, the ports will not be accessible remotely from the outside of the host.
If they would, this would be a security concern due to the default passwords and sometimes no password being used on the applications deployed on minikube and the Kubernetes API itself.

To connect to Keycloak and other services remotely, one way is to use SSH port forwarding.

As Keycloak is quick specific about the configured port and IP address, the port forwarding needs to bind the same port as on minikube.
As it is running on minikube with port 443, this requires running ssh as root so that it can bind port 443 locally.

Given the IP address of minikube on the remote host retrieved by `mininkube ip` with content of `192.168.39.19` the following steps work.

[NOTE]
====
Whenever the minikube instance on the remote host is re-created, it will receive a different IP address and the commands need to be adjusted.
====

. Add an entry to the local `hosts` file that points the host names of minikube:
+
----
127.0.0.1 kubebox.192.168.39.19.nip.io grafana.192.168.39.19.nip.io keycloak.192.168.39.19.nip.io
----

. Put the current user's ssh keys in for the root user, so that `sudo ssh` has access to them.

. Run ssh with port forwarding:
+
----
sudo ssh -L 443:192.168.39.19:443 user@remotehost
----

Now point the browser to \https://keycloak.192.168.39.19.nip.io as usual to interact with the application.
With the SSH tunnel in place, the response times are a bit slower, so users will not be able to run a representative load test with gatling on their local machine and minikube running on the remote machine.
