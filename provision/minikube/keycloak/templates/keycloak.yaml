apiVersion: k8s.keycloak.org/v2alpha1
kind: Keycloak
metadata:
  labels:
    app: keycloak
  name: keycloak
  namespace: {{ .Values.namespace }}
spec:
  hostname:
    {{- if .Values.keycloakHostname }}
    hostname: {{ .Values.keycloakHostname }}
    {{- else }}
    hostname: keycloak-{{ .Values.namespace }}.{{ .Values.hostname }}
    {{- end }}
{{ if .Values.disableIngressStickySession }}
  ingress:
    enabled: true
    annotations:
      haproxy.router.openshift.io/balance: roundrobin
      haproxy.router.openshift.io/disable_cookies: 'true'
{{end}}
  additionalOptions:
{{ if or (eq .Values.database "aurora-postgres") (eq .Values.database "postgres") (eq .Values.database "postgres+infinispan") }}
    - name: db
      value: postgres
    - name: db-url
      value: jdbc:postgresql://postgres:5432/keycloak
{{ else if eq .Values.database "cockroach-single" }}
    - name: db
      value: postgres
    - name: db-url
      value: jdbc:postgresql://cockroach:26257/keycloak?sslmode=disable
{{ else if eq .Values.database "cockroach-operator" }}
    - name: db
      value: postgres
    - name: db-url
      value:  jdbc:postgresql://cockroach-public:26257/keycloak?sslcert=%2Fcockroach%2Fcockroach-certs%2Fclient.root.crt&sslkey=%2Fcockroach%2Fcockroach-certs%2Fclient.root.key&sslmode=verify-full&sslrootcert=%2Fcockroach%2Fcockroach-certs%2Fca.crt&user=root
{{ end }}
{{- if .Values.infinispan.customConfig }}
    - name: cache-config-file
      value: {{ base .Values.infinispan.configFile }}
{{- end }}
{{ if or (eq .Values.database "infinispan") (eq .Values.database "postgres+infinispan") }}
    - name: storage-hotrod-host
      value: infinispan
    - name: storage-hotrod-port
      value: '11222'
    - name: storage-hotrod-username
      value: admin
    - name: storage-hotrod-password
      value: admin
{{ end }}
    - name: db-pool-min-size
      value: {{ quote .Values.dbPoolMinSize }}
    - name: db-pool-max-size
      value: {{ quote .Values.dbPoolMaxSize }}
    - name: db-pool-initial-size
      value: {{ quote .Values.dbPoolInitialSize }}
{{ if .Values.disableCaches }}
    - name: cache-realm-enabled
      value: 'false'
    - name: cache-user-enabled
      value: 'false'
    - name: cache-authorization-enabled
      value: 'false'
    - name: spi-public-key-cache-infinispan-enabled
      value: 'false'
{{ end }}
{{ if eq .Values.storage "jpa+hotrod" }}
    # in the mixed setup we set storage to jpa and override user sessions, auth sessions, single-use objects, and login failures to use hotrod
    - name: storage
      value: jpa
    - name: storage-area-auth-session
      value: hotrod
    - name: storage-area-user-session
      value: hotrod
    - name: storage-area-single-use-object
      value: hotrod
    - name: storage-area-login-failure
      value: hotrod
    - name: storage-deployment-state-version-seed
      value: some-secret-seed
{{ else if ne .Values.storage "" }}
    - name: storage
      value: {{ quote .Values.storage }}
    # this might eventually be set automatically by the operator
    # TODO: value should be stored as a secret instead in a production environment
    # https://github.com/keycloak/keycloak/issues/13377
    - name: storage-deployment-state-version-seed
      value: some-secret-seed
{{ end }}
    - name: log-console-output
      value: json
    - name: metrics-enabled
      value: 'true'
    - name: health-enabled
      value: 'true'
    - name: db-username
      secret:
        name: keycloak-db-secret
        key: username
    - name: db-password
      secret:
        name: keycloak-db-secret
        key: password
{{- if .Values.infinispan.remoteStore.enabled }}
    - name: remote-store-host
      value: {{ quote .Values.infinispan.remoteStore.host }}
    - name: remote-store-port
      value: {{ quote .Values.infinispan.remoteStore.port }}
    - name: remote-store-username
      secret:
        name: remote-store-secret
        key: username
    - name: remote-store-password
      secret:
        name: remote-store-secret
        key: password
{{- end }}
  http:
    tlsSecret: keycloak-tls-secret
  instances: {{ .Values.instances }}
  unsupported:
    podTemplate:
      metadata:
        annotations:
          checksum/config: {{ include (print $.Template.BasePath "/keycloak-providers-configmap.yaml") . | sha256sum }}-{{ include (print $.Template.BasePath "/postgres/postgres-deployment.yaml") . | sha256sum }}-{{ .Values.keycloakImage }}-{{ include (print $.Template.BasePath "/keycloak-infinispan-configmap.yaml") . | sha256sum }}-{{ .Values.otelVersion }}
      spec:
        {{ if .Values.otel }}
        initContainers:
          - name: download-otel
            image: registry.access.redhat.com/ubi8/ubi-minimal
            command:
              - /bin/bash
            args:
              - -c
              # language=bash
              - |
                if [ ! -e /otel/opentelemetry-javaagent-{{ .Values.otelVersion }}.jar ]; then
                  curl -f -L https://github.com/open-telemetry/opentelemetry-java-instrumentation/releases/download/{{ .Values.otelVersion }}/opentelemetry-javaagent.jar -o /otel/opentelemetry-javaagent-{{ .Values.otelVersion }}.jar
                fi
            volumeMounts:
              - name: otel
                mountPath: /otel
        {{ end }}
        containers:
          -
{{ if hasPrefix "localhost/" .Values.keycloakImage }}
            imagePullPolicy: Never
{{ end }}
            env:
              # Limiting Keycloak thread pool as it might lead to an exhaustion of the JGroups thread pool.
              # The combined worker thread pool size of all Keycloak instances shouldn't exceed the size of the JGroups thread pool on each node.
              # After https://issues.redhat.com/browse/ISPN-14780 was fixed in KC 22.0.2 and KC 23, it again defaults to 200.
              # It can be configured by a system property 'jgroups.thread_pool.max_threads' for a different value.
              - name: 'QUARKUS_THREAD_POOL_MAX_THREADS'
                value: {{ div 200 .Values.instances | quote }}
              # This is the queue size on each Keycloak node, that when exceeded will return a 503 response immediately (load shedding).
              # Assuming a Keycloak pod processes around 200 requests per second, a queue of 1000 would lead to maximum waiting times of around 5 seconds.
              # In the future, the queue size could be determined dynamically on each node based on the processing time of previous requests.
              # See: https://github.com/keycloak/keycloak/issues/21962
              - name: 'QUARKUS_THREAD_POOL_QUEUE_SIZE'
                value: '1000'
              # We want to have an externally provided username and password, therefore, we override those two environment variables
              - name: KEYCLOAK_ADMIN
                valueFrom:
                  secretKeyRef:
                    name: keycloak-preconfigured-admin
                    key: username
                    optional: false
              - name: KEYCLOAK_ADMIN_PASSWORD
                valueFrom:
                  secretKeyRef:
                    name: keycloak-preconfigured-admin
                    key: password
                    optional: false
{{ if .Values.otel }}
              # Instrumentation for the HTTP/2 protocol doesn't work yet - no metrics will be available, probably also no tracing
              # As a workaround, force downgrade to HTTP/1.1
              # https://github.com/open-telemetry/opentelemetry-java-instrumentation/issues/8927
              - name: QUARKUS_HTTP_HTTP2
                value: 'false'
              - name: OTEL_SEMCONV_STABILITY_OPT_IN
                value: 'http'
              # https://github.com/open-telemetry/opentelemetry-java-instrumentation
              # https://github.com/open-telemetry/opentelemetry-java/blob/main/sdk-extensions/autoconfigure/README.md
              - name: OTEL_RESOURCE_ATTRIBUTES
                value: service.name=keycloak
              - name: OTEL_TRACES_EXPORTER
                # with otel+tempo 1.4.1 forwarding of traces works, but searching is not returning all values for now, for example delete users was missing
                value: jaeger
              - name: OTEL_EXPORTER_JAEGER_ENDPOINT
                value: http://jaeger-collector.monitoring.svc:14250
              - name: OTEL_TRACES_SAMPLER
                value: parentbased_traceidratio # always_on, parentbased_traceidratio, ...
              - name: OTEL_TRACES_SAMPLER_ARG
                value: {{ .Values.otelSamplingPercentage | quote }}
              - name: OTEL_METRICS_EXPORTER
                value: prometheus
{{ end }}
{{ if eq .Values.storage "jpa" }}
              # avoid a warning when this is not set, see https://github.com/keycloak/keycloak/issues/16616
              - name: KC_SPI_MAP_STORAGE_JPA_LOCK_TIMEOUT
                value: "10000"
{{ end }}
              - name: JAVA_OPTS_APPEND
                # jgroups.thread_dumps_threshold ensure that a log message "thread pool is full" appears once the thread pool is full for the first time
                value: >
                  -Xms{{ .Values.heapInitMB }}m -Xmx{{ .Values.heapMaxMB }}m
                  -XX:MetaspaceSize={{ .Values.metaspaceInitMB }}m -XX:MaxMetaspaceSize={{ .Values.metaspaceMaxMB }}m
                  -Djgroups.thread_dumps_threshold=1
                  -agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=*:8787
{{- if .Values.otel }}
                  -javaagent:/otel/opentelemetry-javaagent-{{ .Values.otelVersion }}.jar
{{- end -}}
{{- if .Values.cryostat }}
                  -Dcom.sun.management.jmxremote.port=9091 -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false
{{- end }}
{{- if .Values.infinispan.site }}
                  -Djboss.site.name={{.Values.infinispan.site}}
{{- end}}
            ports:
{{ if .Values.otel }}
              - containerPort: 9464
                protocol: TCP
                name: otel-prometheus
{{ end }}
{{ if .Values.cryostat }}
              - containerPort: 9091
                protocol: TCP
                name: jfr-jmx
{{ end }}
              - containerPort: 8787
                protocol: TCP
                name: jvmdebug
            resources:
              requests:
                {{ if .Values.cpuRequests }}cpu: "{{ .Values.cpuRequests }}"{{end}}
                {{ if .Values.memoryRequestsMB }}memory: "{{ .Values.memoryRequestsMB }}M"{{end}}
              limits:
                {{ if .Values.cpuLimits }}cpu: "{{ .Values.cpuLimits }}"{{end}}
                {{ if .Values.memoryLimitsMB }}memory: "{{ .Values.memoryLimitsMB }}M"{{end}}
            startupProbe:
              httpGet:
                path: /health/started
                port: 8443
                scheme: HTTPS
              failureThreshold: 600
              initialDelaySeconds: 10
              periodSeconds: 2
            # During Keycloak Infinispan view updates for members leaving and rebalancing, there is an increased latency for all requests, observed with up to 10 seconds.
            # With all requests being queued, also the liveness probe is queue, and is therefore slow.
            # In a high-load or even overload scenario, the probes will be queued in the executor thread pool, and won't return in time.
            # With load shedding activated when requests are rejected from the executor thread pool, failing readiness probes will lead to Pods not receiving any load for
            # a period or time, and with failing liveness probes the Pods will eventually be restarted. So the best way to run Keycloak in Kubernetes would be to disable
            # those probes for now.
            # https://github.com/keycloak/keycloak/issues/22109
            readinessProbe:
              exec:
                command:
                  - 'true'
            livenessProbe:
              exec:
                command:
                  - 'true'
            volumeMounts:
              - name: keycloak-providers
                mountPath: /opt/keycloak/providers
                readOnly: true
{{ if .Values.infinispan.customConfig }}
              - name: kcb-infinispan-cache-config
                mountPath: /opt/keycloak/conf/{{ base .Values.infinispan.configFile }}
                subPath: {{ base .Values.infinispan.configFile }}
                readOnly: true
{{ end }}
{{ if .Values.otel }}
              - name: otel
                mountPath: /otel
                readOnly: true
{{ end }}
{{ if eq .Values.database "cockroach-operator" }}
              - name: client-certs
                mountPath: /cockroach/cockroach-certs
{{ end }}
        volumes:
          - name: keycloak-providers
            configMap:
              name: keycloak-providers
{{ if .Values.infinispan.customConfig }}
          - name: kcb-infinispan-cache-config
            configMap:
              name: kcb-infinispan-cache-config
              items:
                - key: {{base .Values.infinispan.configFile}}
                  path: {{base .Values.infinispan.configFile}}
{{ end }}
{{ if .Values.otel }}
          - name: otel
            persistentVolumeClaim:
              claimName: otel
{{ end }}
{{ if eq .Values.database "cockroach-operator" }}
          - name: client-certs
            projected:
              sources:
                - secret:
                    name: cockroach-node
                    items:
                      - key: ca.crt
                        path: ca.crt
                - secret:
                    name: cockroach-root-jdbc
                    items:
                      - key: tls.crt
                        path: client.root.crt
                      - key: tls.pk8
                        path: client.root.key
{{ end }}
