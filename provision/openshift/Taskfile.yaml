# https://taskfile.dev

version: '3'

vars:
  KC_HOSTNAME_SUFFIX: '{{default "$(kubectl get route/console -n openshift-console -o jsonpath=\u0027{.spec.host}\u0027 | cut -d . -f 2-)" .KC_HOSTNAME_SUFFIX}}'
  KC_NAMESPACE_PREFIX: '{{default "$(whoami)-" .KC_NAMESPACE_PREFIX}}'
  KC_ADMIN_PASSWORD: '{{default "$(aws secretsmanager get-secret-value --region eu-central-1 --secret-id keycloak-master-password --query SecretString --output text --no-cli-pager || echo admin)" .KC_ADMIN_PASSWORD}}'
  KC_JGROUPS_TLS: '{{default "true" .KC_JGROUPS_TLS}}'

output: prefixed

dotenv: ['.env']

includes:
  common: ../common
  keycloak:
    taskfile: ../keycloak-tasks/Utils.yaml
    dir: ../keycloak-tasks/
    internal: true

tasks:
  default:
    deps:
      - gatlinguser
    cmds:
      - echo Keycloak is ready for load testing!
      - bash -c ./isup.sh
    silent: true

  reset-keycloak:
    deps:
      - common:split
    cmds:
      - bash -c "kubectl delete keycloaks.k8s.keycloak.org/keycloak -n {{.KC_NAMESPACE_PREFIX}}keycloak || exit 0"
      - bash -c "kubectl delete deployment/keycloak-operator -n {{.KC_NAMESPACE_PREFIX}}keycloak || exit 0"
      - bash -c "kubectl delete deployment/{{.KC_DATABASE}} -n {{.KC_NAMESPACE_PREFIX}}keycloak || exit 0"
      - kubectl wait --timeout=30s -n keycloak --for delete pod --selector=app=keycloak
      - kubectl wait --timeout=30s -n keycloak --for delete pod --selector=app.kubernetes.io/name=keycloak-operator
      - kubectl wait --timeout=30s -n keycloak --for delete pod --selector=app=postgres
      - >
        bash -c '
        if [ "{{.KC_DATABASE}}" == "aurora-postgres" ]; then
          kubectl apply -f reset_aurora_postgres.yaml -n {{.KC_NAMESPACE_PREFIX}}keycloak
          kubectl wait --timeout=30s --for=condition=complete job/reset-aurora-postgres -n {{.KC_NAMESPACE_PREFIX}}keycloak
          kubectl delete -f reset_aurora_postgres.yaml -n {{.KC_NAMESPACE_PREFIX}}keycloak
        fi
        '
      # discard status of keycloak to force redeployment
      - rm -f .task/checksum/keycloak
      # discard status of gatling user to force redeployment
      - rm -f .task/checksum/gatlinguser


      - task: default

  openshift-env:
    deps:
      - common:split
    cmds:
      # create marker files that can then be checked in other tasks
      - mkdir -p .task
      - echo {{.KC_HOSTNAME_SUFFIX}} > .task/var-KC_HOSTNAME_SUFFIX
      - echo {{.KC_NAMESPACE_PREFIX}} > .task/var-KC_NAMESPACE_PREFIX
      - echo {{.KC_ADMIN_PASSWORD}} > .task/var-KC_ADMIN_PASSWORD
      - echo {{.KC_ISPN_NAMESPACE}} > .task/var-KC_ISPN_NAMESPACE
    run: once
    sources:
      - .task/subtask-{{.TASK}}.yaml
    status:
      - test "{{.KC_HOSTNAME_SUFFIX}}" == "$(cat .task/var-KC_HOSTNAME_SUFFIX)"
      - test "{{.KC_NAMESPACE_PREFIX}}" == "$(cat .task/var-KC_NAMESPACE_PREFIX)"
      - test "{{.KC_ADMIN_PASSWORD}}" == '$(cat .task/var-KC_ADMIN_PASSWORD)'
      - test "{{.KC_ISPN_NAMESPACE}}" == '$(cat .task/var-KC_ISPN_NAMESPACE)'

  dataset-import:
    deps:
      - gatlinguser
    cmds:
      - bash -c "../../dataset/dataset-import.sh -l https://keycloak-{{.KC_NAMESPACE_PREFIX}}keycloak.{{.KC_HOSTNAME_SUFFIX}}/realms/master/dataset {{.CLI_ARGS}}"
    silent: true

  gatlinguser:
    deps:
      - common:keycloak-cli-unzip
      - common:tlsdisableagent
      - keycloak
      - common:split
    env:
      KC_OPTS: "-javaagent:../tlsdisableagent/tlscheckdisable-agent.jar"
      KEYCLOAK_HOME: "../keycloak-cli/keycloak"
    cmds:
      - bash -c ./isup.sh
      - >
        bash -c '
        if [ "{{.KC_HOSTNAME_SUFFIX}}" != "" ];
          then ../keycloak-cli/keycloak/bin/kcadm.sh config credentials --server https://keycloak-{{.KC_NAMESPACE_PREFIX}}keycloak.{{.KC_HOSTNAME_SUFFIX}}/ --realm master --user admin --password "{{.KC_ADMIN_PASSWORD}}";
          else echo -e "KC_HOSTNAME_SUFFIX value is not set properly";
        fi'
      - bash -c "../../benchmark/src/main/content/bin/initialize-benchmark-entities.sh -r test-realm -d"
    sources:
      - ../../benchmark/src/main/content/bin/initialize-benchmark-entities.sh
      - .task/subtask-{{.TASK}}.yaml
      # if keycloak's database deployment changes, this restarts the DB and the Gatling user needs to be re-created
      - .task/status-keycloak-db.json
      - .task/var-KC_DATABASE

  jaeger:
    deps:
      - common:split
      - common:env
      - openshift-env
      - grafana-sa
    env:
      KB_RETENTION: '{{.KB_RETENTION}}'
    cmds:
      - helm repo add jaegertracing https://jaegertracing.github.io/helm-charts
      - helm repo update
      - helm upgrade --install jaeger jaegertracing/jaeger --version 0.58.0 -n monitoring -f ../minikube/jaeger/values.yaml
      - envsubst < ../minikube/jaeger/deployment-patch.yaml > .task/subtask-{{.TASK}}-patchfile.yaml
      - kubectl patch deployment jaeger -n monitoring --patch-file .task/subtask-{{.TASK}}-patchfile.yaml
    sources:
      - jaeger/**/*.*
      - .task/subtask-{{.TASK}}.yaml
      - .task/var-KB_RETENTION
      - .task/var-KC_HOSTNAME_SUFFIX

  grafana-sa:
    # this setup was inspired by https://zhimin-wen.medium.com/custom-grafana-dashboard-for-user-workload-in-openshift-6dc2d4cad274
    deps:
      - common:split
      - common:env
      - openshift-env
    cmds:
      - kubectl create namespace monitoring || true
      # this creates the service account outside the Helm chart to be able to create the token which is later an input of the Helm change
      - oc create sa grafana -n monitoring || exit 0
      - oc adm policy add-cluster-role-to-user cluster-monitoring-view -z grafana -n monitoring
      - kubectl get project/monitoring -o jsonpath='{.metadata.annotations.openshift\.io/sa\.scc\.uid-range}' |  cut -f1 -d"/" > .task/monitoring-uids
      - kubectl apply -f grafana-sa-secret.yaml -n monitoring
    sources:
      - .task/subtask-{{.TASK}}.yaml
      - .task/var-KC_HOSTNAME_SUFFIX
      - grafana-sa-secret.yaml
    generates:
      - .task/monitoring-uids

  grafana:
    deps:
      - common:split
      - common:env
      - openshift-env
      - grafana-sa
    cmds:
      - helm repo add grafana https://grafana.github.io/helm-charts
      - helm repo update
      - >
        helm -n monitoring upgrade --install grafana grafana/grafana --version 6.55.0 -f grafana.yaml
        --set ingress.hosts[0]=grafana.{{.KC_HOSTNAME_SUFFIX}}
        --set ingress.annotations."route\.openshift\.io/termination"=edge
        --set ingress.annotations."route\.openshift\.io/insecureEdgeTerminationPolicy"=Redirect
        --set securityContext.runAsUser=$(cat .task/monitoring-uids)
        --set securityContext.runAsGroup=$(cat .task/monitoring-uids)
        --set securityContext.fsGroup=$(cat .task/monitoring-uids)
        --set adminPassword="{{.KC_ADMIN_PASSWORD}}"
    sources:
      - .task/subtask-{{.TASK}}.yaml
      - .task/monitoring-uids
      - .task/var-KC_HOSTNAME_SUFFIX
      - grafana.yaml

  monitoring:
    deps:
      - common:split
      - common:env
      - openshift-env
      - grafana
      - jaeger
    cmds:
      - helm upgrade --install monitoring monitoring
    sources:
      - monitoring/**/*.*
      - .task/subtask-{{.TASK}}.yaml
      - .task/var-KC_HOSTNAME_SUFFIX

  keepalive:
    deps:
      - openshift-env
    cmds:
      - kubectl create namespace keepalive
    sources:
      - .task/subtask-{{.TASK}}.yaml

  infinispan-remote:
    internal: true
    silent: true
    deps:
      - openshift-env
    cmds:
      - |
        if [ "{{.KC_ISPN_NAMESPACE}}" != "" ]; then
        echo "true" > .task/remote-store-enabled
        echo "infinispan.{{.KC_ISPN_NAMESPACE}}.svc" > .task/remote-store-host
        oc -n {{.KC_ISPN_NAMESPACE}} get secrets connect-secret -o 'jsonpath={.data.identities\.yaml}' | base64 -d | yq .credentials[0].password > .task/remote-store-password
        exit 0
        fi
        echo "false" > .task/remote-store-enabled
        echo "" > .task/remote-store-host
        echo "" > .task/remote-store-password
        exit 0
    generates:
      - .task/remote-store-enabled
      - .task/remote-store-host
      - .task/remote-store-password
    sources:
      - .task/var-KC_ISPN_NAMESPACE

  keycloak:
    deps:
      - common:datasetprovider
      - common:split
      - common:env
      - openshift-env
      - infinispan-remote
    vars:
      CURRENT_KC_CONTAINER_IMAGE: '{{ ternary "$(cat ../keycloak-tasks/.task/var-CUSTOM_CONTAINER_IMAGE_FILE 2> /dev/null || echo \"\")" .KC_CONTAINER_IMAGE (empty .KC_CONTAINER_IMAGE) }}'
    cmds:
      # again
      - task: '{{if .KC_REPOSITORY}}keycloak:prepare-custom-images{{else}}keycloak:no-op{{end}}'
        vars:
          NAMESPACE: "{{.KC_NAMESPACE_PREFIX}}keycloak"
          ROSA_CLUSTER_NAME: "current"
      - task: keycloak:install-keycloak-operator
        vars:
          NAMESPACE: "{{.KC_NAMESPACE_PREFIX}}keycloak"
      - >
        helm upgrade --install keycloak --namespace {{.KC_NAMESPACE_PREFIX}}keycloak
        --set hostname={{.KC_HOSTNAME_SUFFIX}}
        --set keycloakHostname={{.KC_HOSTNAME_OVERRIDE}}
        --set keycloakHealthHostname={{.KC_HEALTH_HOSTNAME}}
        --set otel={{.KC_OTEL}}
        --set otelSamplingPercentage={{.KC_OTEL_SAMPLING_PERCENTAGE}}
        --set dbPoolInitialSize={{.KC_DB_POOL_INITIAL_SIZE}}
        --set dbPoolMinSize={{.KC_DB_POOL_MIN_SIZE}}
        --set dbPoolMaxSize={{.KC_DB_POOL_MAX_SIZE}}
        --set dbUrl={{ .KC_DATABASE_URL }}
        --set database={{.KC_DATABASE}}
        --set keycloakImage={{.CURRENT_KC_CONTAINER_IMAGE}}
        --set instances={{ .KC_INSTANCES }}
        --set cpuRequests={{ .KC_CPU_REQUESTS }}
        --set cpuLimits={{ .KC_CPU_LIMITS }}
        --set memoryRequestsMB={{ .KC_MEMORY_REQUESTS_MB }}
        --set memoryLimitsMB={{ .KC_MEMORY_LIMITS_MB }}
        --set heapInitMB={{ .KC_HEAP_INIT_MB }}
        --set heapMaxMB={{ .KC_HEAP_MAX_MB }}
        --set metaspaceInitMB={{ .KC_METASPACE_INIT_MB }}
        --set metaspaceMaxMB={{ .KC_METASPACE_MAX_MB }}
        --set infinispan.jgroupsTls={{ .KC_JGROUPS_TLS }}
        --set infinispan.customConfig={{ .KC_CUSTOM_INFINISPAN_CONFIG }}
        --set infinispan.configFile={{ .KC_CUSTOM_INFINISPAN_CONFIG_FILE }}
        --set infinispan.remoteStore.enabled=$(cat .task/remote-store-enabled)
        --set infinispan.remoteStore.host=$(cat .task/remote-store-host)
        --set infinispan.remoteStore.password=$(cat .task/remote-store-password)
        --set infinispan.site={{.KC_NAMESPACE_PREFIX}}keycloak
        --set cryostat={{ .KC_CRYOSTAT }}
        --set sqlpad=false
        --set environment=openshift
        --set namespace={{.KC_NAMESPACE_PREFIX}}keycloak
        --set keycloakAdminPassword="{{.KC_ADMIN_PASSWORD}}"
        --set disableIngressStickySession={{ .KC_DISABLE_STICKY_SESSION }}
        --set nodePortsEnabled=false
        ../minikube/keycloak
      - >
        bash -c '
        if [[ "{{.KC_DATABASE}}" != "none" && "{{.KC_DATABASE}}" != "aurora-postgres" ]];
        then kubectl get deployment/{{.KC_DATABASE}} -n {{.KC_NAMESPACE_PREFIX}}keycloak -o=jsonpath="{.spec}" > .task/status-{{.TASK}}-db.json;
        else echo "none" > .task/status-{{.TASK}}-db.json;
        fi'
      # kill all CrashLoopBackOff and ImagePullBackOff pods to trigger a fast restart and not wait Kubernetes
      - bash -c 'kubectl get pods -n {{.KC_NAMESPACE_PREFIX}}keycloak | grep -E "(BackOff|Error|ErrImageNeverPull|InvalidImageName)" | tr -s " " | cut -d" " -f1 | xargs -r -L 1 kubectl delete -n {{.KC_NAMESPACE_PREFIX}}keycloak pod'
      # wait a bit for the operator to pick up the changes
      - bash -c 'sleep 2'
      - ./isup.sh
      # remove all no longer used images from minikube to preserve disk space
    sources:
      - ../minikube/keycloak/**/*.*
      - .task/subtask-{{.TASK}}.yaml
      - .task/var-KC_*
      - .task/remote-store-enabled
      - .task/remote-store-host
      - .task/remote-store-username
      - .task/remote-store-password
