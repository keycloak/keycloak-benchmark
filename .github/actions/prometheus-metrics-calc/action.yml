name: Execute metrics calculation
description: Execute metrics calculation

inputs:
  input:
    description: 'The file name containing result numbers.'
    required: true
  performedTestName:
    description: 'The name of the performed test to include in JSON file.'
    required: true
  calculatedMetricName:
    description: 'The name of the metric that is calculated for including in JSON file.'
    default: '1vCPU'
  criteriaValue:
    description: 'The value for the criteria used during benchmark run.'
    required: true
  isvCPU:
    description: 'Defines if vCPU metric is calculated.'
    default: 'true'
  isMemory:
    description: 'Defines if memory metric is calculated.'
    default: 'false'
  replicas:
    description: 'Number of pods on which the metric is calculated.'
    default: '3'
  measurementInterval:
    description: 'A specific time period over which we want to calculate the required metrics.'

runs:
  using: composite
  steps:
    - id: calculate-vcpu-metrics
      name: Calculate the vCPU metric and store environment variable.
      if: ${{ inputs.isvCPU == 'true' }}
      env:
        POD_NUM: ${{ inputs.replicas }}             #redefining the input parameters as environment variables due to math operations below
        CRITERIA_VALUE: ${{ inputs.criteriaValue }}
        TIME_INTERVAL: ${{ inputs.measurementInterval }}
      shell: bash
      # language=bash
      run: |
        readarray -t lines < ${{ inputs.input }}
        num1=${lines[0]}
        num2=${lines[1]}
        #calculating the difference of cumulative metric (changed during the benchmark execution)
        difference=$(awk "BEGIN {print ($num2-$num1); exit}")
        #the script calculates vCPU need to calculate the vcpu number from CPU seconds metrics for the interval during which the test was running
        metric_count_in_interval=$(awk "BEGIN {print $difference/$TIME_INTERVAL; exit}")
        #calculating the average metric per pod
        metric_per_pod=$(awk "BEGIN {print $metric_count_in_interval/$POD_NUM; exit}")
        #Calculating the final number, i.e. how many of specified criteria (e.g. user logins/sec, client credential grants, etc)
        #can be handled with requested metric per pod. The result is number rounded down.
        result=$(awk "BEGIN {print int($CRITERIA_VALUE/$metric_per_pod); exit}")
        echo "CALCULATED_METRIC_VALUE=$result" >> $GITHUB_ENV

    - id: calculate-memory-metric
      name: Calculate the memory metric and store in environment variable.
      if: ${{ inputs.isMemory == 'true' }}
      env:
        POD_NUM: ${{ inputs.replicas }}             #redefining the input parameters as environment variables due to math operations below
        CRITERIA_VALUE: ${{ inputs.criteriaValue }}
        TIME_INTERVAL: ${{ inputs.measurementInterval }}
      shell: bash
      # language=bash
      run: |
        readarray -t lines < ${{ inputs.input }}
        num1=${lines[0]}
        num2=${lines[1]}
        #calculating the difference of cumulative metric (changed during the benchmark execution)
        difference=$(awk "BEGIN {print ($num2-$num1); exit}")
        #calculating the average metric per pod
        metric_per_pod=$(awk "BEGIN {print $difference/$POD_NUM; exit}")
        #Calculating the final number, i.e. based on current environment setup how many of specified criteria (e.g. user active session, etc)
        #can be handled with 500Mb per pod based on the number calculated above. The result is number rounded down.
        result=$(awk "BEGIN {print int($CRITERIA_VALUE*500/$metric_per_pod); exit}")
        echo "CALCULATED_METRIC_VALUE=$result" >> $GITHUB_ENV

    - id: metric-store-in-result-file
      name: Stores got information in JSON file.
      env:
        CALCULATED_METRIC_VALUE: ${{ env.CALCULATED_METRIC_VALUE }}
        HORREUM_OUTPUT_FILE_NAME: ${{ env.HORREUM_OUTPUT_FILE_NAME }}
      shell: bash
      # language=bash
      run: |
        #Storing all information in JSON using jq library
        jq --arg testName "${{ inputs.performedTestName }}" --arg key "${{ inputs.calculatedMetricName }}" \
        --arg val ${CALCULATED_METRIC_VALUE} '. + {($testName): {($key):($val|tonumber)}}' ${HORREUM_OUTPUT_FILE_NAME} > tmp.json && \
        mv tmp.json ${HORREUM_OUTPUT_FILE_NAME}

    - id: gatling-report-parse-and-store
      name: Parses gatling report and stores got information in JSON file.
      env:
        HORREUM_OUTPUT_FILE_NAME: ${{ env.HORREUM_OUTPUT_FILE_NAME }}
      shell: bash
      # language=bash
      run: |
        gatlingReportPath="ansible/files/benchmark/*/results/"
        gatlingReportDirName=$(ls -lrt $gatlingReportPath | tail -1 | awk '{print $9}')
        gatlingReportFilePath="$gatlingReportPath$gatlingReportDirName/js/stats.json"
        statsJson=$(jq '.. | .stats? | select( . != null )' $gatlingReportFilePath | jq -s .)
        #Putting environment parameters into JSON
        jq --argjson statsJson "${statsJson}" --arg testName "${{ inputs.performedTestName }}" \
        '.[$testName].statistics |= .+ ($statsJson)' ${HORREUM_OUTPUT_FILE_NAME} > tmp.json && \
        mv tmp.json ${HORREUM_OUTPUT_FILE_NAME}
        #Reading ispn metrics file
        readarray -t lines < ispn_metrics_file
        for i in "${lines[@]}"
        do
            IFS=',' read -ra metric <<< "$i"
            #Putting metric into JSON
            jq --arg cacheName "${metric[0]}" --arg val "${metric[1]}" --arg testName "${{ inputs.performedTestName }}" \
            '.[$testName].ispnStatistics |= .+ {"cacheName": ($cacheName), "averageXsiteReplicationTime":($val|tonumber)}' ${HORREUM_OUTPUT_FILE_NAME} > tmp.json && \
            mv tmp.json ${HORREUM_OUTPUT_FILE_NAME}
        done
